# This is old not working Danbooru downloader script. It will be deleted soon.

from bs4 import BeautifulSoup
import os
import sys
import time
import urllib.request

tags = input("Enter tags: ").strip()
limit = eval(input("Enter limit: "))
url = "http://gelbooru.com/index.php?page=dapi&s=post&q=index&tags={}&limit={}".format(tags,limit)

r = urllib.request.urlopen(url)
soup = BeautifulSoup(r)

links = {}

for post in soup.findAll('post'):
	post_attrs = dict(post.attrs)
	url = post_attrs['file_url']
	uid = post_attrs['id']
	links[uid] = url

def reporthook(count, block_size, total_size):
		global start_time
		if count == 0:
			start_time = time.time()
			return
		duration = time.time() - start_time
		progress_size = int(count * block_size)
		speed = int(progress_size / (1024 * duration))
		percent = min(int(count * block_size * 100 / total_size),100)
		if (progress_size / 1024) < 1024: 
			progress_size = str(progress_size / (1024)) + " KB downloaded"
		else:
			progress_size = str(progress_size / (1024 * 1024)) + " MB downloaded"
			sys.stdout.write("\r%d%%, %s, %d KB/s, %d seconds passed" %
		    (percent, progress_size, speed, duration))
			sys.stdout.flush()
			
def save(url, filename):
	urllib.request.urlretrieve(url, filename, reporthook)
	print()

if __name__ == '__main__':
	newpath = '{}'.format(tags).split("+")[0]
	try:  
		os.makedirs(newpath)
	except FileExistsError:
		print("File already exist. Overwriting...")
		pass
	os.chdir(newpath)
	for each in links:
		try:
			save(links[each], each + "." + links[each].split(".")[-1])
		except urllib.request.ContentTooShortError:
			print("\nDownload incomplete. Please try again.")
	
